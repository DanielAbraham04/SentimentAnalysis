{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from statistics import mean, stdev, median, mode\n",
    "# With PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "qXIBsqXjpZdd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from autocorrect import Speller\n",
    "# from pycontractions import Contractions\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "# from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "w8gGSNnE_Ngv"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "CmfT_d2dpZdn"
   },
   "outputs": [],
   "source": [
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "UJso8xCYpZdq"
   },
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "0yxaZ3DHpZdt"
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/danielabraham/Downloads'\n",
    "def text_preprocessing(data):\n",
    "    \n",
    "    # This method replaces two or more consecutive letters with the same character to something shorter. For example, gooooooood becomes good.\n",
    "    def replaceTwoOrMore(s):\n",
    "        #look for 2 or more repetitions of character\n",
    "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "        return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "    # This method converts camel cased words into space delimited words.\n",
    "    # For example: ThisIsASentence will be changed to This Is A Sentence\n",
    "    def convertCamelCase(word):\n",
    "        return re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",word)\n",
    "\n",
    "    # Read a flat file containing some abbreviations and their expansions in pipe separated format\n",
    "    # Use these abbreviations to replace text in the tweets as part of Preprocessing\n",
    "    \n",
    "    def readAbbrFile(abb_path):\n",
    "        global abbr_dict\n",
    "        abbr_dict ={}\n",
    "        f = open(abb_path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for i in lines:\n",
    "            tmp = i.split('|')\n",
    "            abbr_dict[tmp[0]] = tmp[1]\n",
    "\n",
    "        return abbr_dict\n",
    "  \n",
    "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
    "    # and replaces the key with the corresponding value\n",
    "    def replaceAbbr(s):\n",
    "        temp = \" \".join([abbr_dict[word.lower()] if word.lower() in abbr_dict.keys() else word for word in s.split()])\n",
    "        return temp\n",
    "   \n",
    "\n",
    "    def readcontractions(contra_path):\n",
    "        global contra_dict\n",
    "        contra_dict ={}\n",
    "        f = open(contra_path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for i in lines:\n",
    "            try: \n",
    "                tmp = i.replace('\"', '').replace(',', '').replace('\\n', ' ').split(':')\n",
    "                contra_dict[tmp[0]] = tmp[1]\n",
    "            except:\n",
    "                print(tmp)\n",
    "                print(z)\n",
    "\n",
    "        return contra_dict\n",
    "    \n",
    "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
    "    # and replaces the key with the corresponding value\n",
    "    def replacecontra(s):\n",
    "        temp = \" \".join([contra_dict[word.lower()] if word.lower() in contra_dict.keys() else word for word in s.split()])\n",
    "        return temp\n",
    "     \n",
    "    \n",
    "\n",
    "    abb_path = os.path.join(data_path,\"abbrevations.txt\")\n",
    "    abbr_dict = readAbbrFile(abb_path)\n",
    "    \n",
    "    contra_path = os.path.join(data_path,\"contractions.txt\")\n",
    "    contra_dict = readcontractions(contra_path)\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('[^\\x00-\\x7f]', ' ') # remove non ascii characters\n",
    "    \n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: x.lower())\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: replaceAbbr(x))\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: replacecontra(x))\n",
    "\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',' ') #remove URL\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('(\\s)@\\w+', ' ') #remove usernames\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('@\\w+', ' ') #remove usernames\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('<[^<]+?>', ' ') #remove HTML tags\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('[<>!#@$:.,%\\?-_]+', ' ') #remove punctuation and special characters\n",
    "    \n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('\\d+', ' ') # removing the words with more than 1 digit\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('\\n\\n', ' ')\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('\\n', ' ') # removing new line characters\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('[^\\w\\s]',' ')\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('\\s+',' ')  \n",
    "\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: replaceTwoOrMore(x))\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: convertCamelCase(x))\n",
    "\n",
    "    # Remove stop words from text\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_list]))\n",
    "\n",
    "\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: \" \".join([re.sub(r\"\\b(?:a{0,2}h{1,2}a{0,2}){2,}h?\\b\", \"haha\", word) for word in x.split()])) # REPLACING HAHAHAH WITH HAHA\n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: \" \".join([re.sub(r\"\\b(?:o{0,2}l{1,2}o{0,2}){2,}l?\\b\", \"lol\", word) for word in x.split()])) # REPLACING lOLOLOL WITH LOL\n",
    "                                                  \n",
    "    data[\"RawTweet\"] = data[\"RawTweet\"].apply(lambda x: lemmatize_sentence(x))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "DgyAcMZRpZdv"
   },
   "outputs": [],
   "source": [
    "def create_vocab(data1, data2):\n",
    "    temp1 = text_preprocessing(data1)\n",
    "    temp2 = text_preprocessing(data2)\n",
    "    return temp1, temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "YcIgGnBHpZd1",
    "outputId": "185586a2-72e7-46f6-8858-9c66f561fdad"
   },
   "outputs": [],
   "source": [
    "\n",
    "training_file = \"/Users/danielabraham/Downloads/training-Obama-Romney-tweets1.xlsx\"\n",
    "obama_train1_temp = pd.read_excel(training_file,sheet_name='Obama',usecols=\"D,E\", skiprows=[0])\n",
    "romney_train1_temp = pd.read_excel(training_file,sheet_name='Romney',usecols=\"D,E\", skiprows=[0])\n",
    "\n",
    "obama_train1_temp.columns = ['RawTweet', 'Class']\n",
    "romney_train1_temp.columns = ['RawTweet', 'Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3fSFxgW_NhI"
   },
   "source": [
    "## Removing datapoints with mixed sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "W-c-9PNk_NhJ"
   },
   "outputs": [],
   "source": [
    "obama_train1 = obama_train1_temp[obama_train1_temp['Class'] .isin((1,-1,0))]\n",
    "romney_train1 = romney_train1_temp[romney_train1_temp['Class'] .isin((1,-1,0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_train1 = obama_train1.dropna()\n",
    "romney_train1 = romney_train1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j9EdPb9_Nha"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "Qsh4tjZk_Nhb"
   },
   "outputs": [],
   "source": [
    "stop_list = stopwords.words('english')\n",
    "stop_list.extend(['rt', 'retweet', 'e'])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "yZD3LoJ1_Nhe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:66: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('[^\\x00-\\x7f]', ' ') # remove non ascii characters\n",
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:72: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',' ') #remove URL\n",
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:73: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('(\\s)@\\w+', ' ') #remove usernames\n",
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:74: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('@\\w+', ' ') #remove usernames\n",
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:75: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('<[^<]+?>', ' ') #remove HTML tags\n",
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:76: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('[<>!#@$:.,%\\?-_]+', ' ') #remove punctuation and special characters\n",
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:78: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('\\d+', ' ') # removing the words with more than 1 digit\n",
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:81: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('[^\\w\\s]',' ')\n",
      "/var/folders/rq/n9728_052hv_9kgv7hx0cf0r0000gn/T/ipykernel_56212/724192422.py:82: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"RawTweet\"] = data[\"RawTweet\"].str.replace('\\s+',' ')\n"
     ]
    }
   ],
   "source": [
    "obama_train1_pr, romney_train1_pr = create_vocab(obama_train1, romney_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RawTweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>except woman work wh make less honest yup obam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>day election selection elect lewis ken hall mh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>reason ann romney michelle obama match last ni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>obama kenakan cincin syahadat sejak sma</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>bitch like obama bitch want food stamp lmao</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5470 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               RawTweet Class\n",
       "0     kirkpatrick wear baseball cap embroider obama ...     0\n",
       "2     obama debate cracker as cracker tonight tune t...     1\n",
       "4     miss point afraid understand big picture dont ...     0\n",
       "6     raise democrat leave party year ago lifetime n...    -1\n",
       "7     obama camp can not afford low expectation toni...     0\n",
       "...                                                 ...   ...\n",
       "7190  except woman work wh make less honest yup obam...     0\n",
       "7192  day election selection elect lewis ken hall mh...     1\n",
       "7193  reason ann romney michelle obama match last ni...     0\n",
       "7194            obama kenakan cincin syahadat sejak sma     0\n",
       "7195        bitch like obama bitch want food stamp lmao     0\n",
       "\n",
       "[5470 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train1_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nyt2ltYpZd5"
   },
   "source": [
    "\n",
    "## Saving Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "dugwt7lTaCiA"
   },
   "outputs": [],
   "source": [
    "obama_train1_pr.to_csv(os.path.join(data_path, 'Obama_Training_Data_Preprocessed.csv'))\n",
    "romney_train1_pr.to_csv(os.path.join(data_path, 'Romney_Training_Data_Preprocessed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_Sentiment_Analysis_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
